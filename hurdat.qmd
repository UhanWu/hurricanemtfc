---
title: "hurdat"
format: html
editor: visual
---

```{r}
csv = read.csv("hurdat2-1.txt", header=F, as.is=T)

names(csv) = c("DATE", "TIME_UTC", "POINT_TYPE", "STATUS", 
	"LATITUDE", "LONGITUDE", "WINDSPEED_KT", "PRESURE_MB", 
	"NE_34KT", "SE_34KT", "NW_34_KT", "SW_34_KT",
	"NE_50KT", "SE_50KT", "NW_50_KT", "SW_50_KT",
	"NE_64KT", "SE_64KT", "NW_64_KT", "SW_64_KT", "EyeRadii")

panel = cbind(HID = NA, HNAME = NA, csv)

panel$HID = ifelse(grepl("AL|EP|CP", panel$DATE), panel$DATE, NA)

panel$HNAME = ifelse(grepl("AL|EP|CP", panel$DATE), panel$TIME_UTC, NA)

library(zoo)

panel$HID = na.locf(panel$HID)

panel$HNAME = na.locf(panel$HNAME)

panel = panel[!grepl("AL|EP|CP", panel$DATE), ]


hurricanedf
```

```{r}
hurricanedf |>
  filter(HNAME == "IDA")
```

```{r}



hurricanedf <- read_csv("dfhurdat.csv")

recenthurdf <- hurricanedf|>
  filter(EyeRadii != -999) |>
  filter(POINT_TYPE == "L") |>
  filter(NW_34_KT != -999) |>
  mutate(Max_34KT = pmax(NE_34KT, SE_34KT, NW_34_KT, SW_34_KT)) |>
  select(year, HNAME, DATE, TIME_UTC, POINT_TYPE, LATITUDE, LONGITUDE, 
         WINDSPEED_KT, EyeRadii, Max_34KT, PRESURE_MB) |>
  group_by(HNAME) |>
  mutate(selection1 =  max(WINDSPEED_KT)) |>
  ungroup() |>
  filter(WINDSPEED_KT==selection1) |>
  group_by(HNAME) |>
  mutate(selection2 =  max(Max_34KT)) |>
  ungroup() |>
  filter(Max_34KT==selection2) |>
  group_by(HNAME) |>
  mutate(selection3 =  max(PRESURE_MB)) |>
  ungroup() |>
  filter(PRESURE_MB==selection3) |>
  select(!c(selection1, selection2, selection3))
    

write_csv(recenthurdf, "recenthurdf.csv")
```

```{r}

#MAX WINDSPEED
maxwinddf <- hurricanedf |>
  filter(POINT_TYPE == "L") |>
  mutate(Max_34KT = pmax(NE_34KT, SE_34KT, NW_34_KT, SW_34_KT)) |>
  summarise(maxwind = max(WINDSPEED_KT), maxEyeRadii = max(EyeRadii), 
            maxdist = max(Max_34KT), .by = c(HID)) |>
  summarise(maxwind = max(maxwind), maxEyeRadii = max(maxEyeRadii), maxdist = max(maxdist), .by = c(HID)) |>
  filter(maxwind != -999 ) |>
  filter(maxwind != 0 )  

# 
#   ggplot(aes(x = maxwind)) + 
#   geom_histogram(binwidth = 10, color = "white")


min_value <- min(maxwinddf$maxwind)
max_value <- max(maxwinddf$maxwind)
desired_interval_size <- 10 # Change this to your desired interval size
breaks_number <- (max_value - min_value) / desired_interval_size

hist(maxwinddf$maxwind, prob = TRUE, breaks = breaks_number)

lines(density(maxwinddf$maxwind), col = "red")


# MAXEYERADII
eyeradiidf <- hurricanedf |>
  filter(POINT_TYPE == "L") |>
  mutate(Max_34KT = pmax(NE_34KT, SE_34KT, NW_34_KT, SW_34_KT)) |>
  summarise(maxwind = max(WINDSPEED_KT), maxEyeRadii = max(EyeRadii), 
            maxdist = max(Max_34KT), .by = c(HID)) |>
  summarise(maxwind = max(maxwind), maxEyeRadii = max(maxEyeRadii), maxdist = max(maxdist), .by = c(HID)) |>
  filter(maxEyeRadii != -999 ) |>
  filter(maxEyeRadii != 0 ) 




  # ggplot(aes(x = maxEyeRadii)) + 
  # geom_histogram(binwidth = 5, color = "white")


min_value <- min(eyeradiidf$maxEyeRadii)
max_value <- max(eyeradiidf$maxEyeRadii)
desired_interval_size <- 10 # Change this to your desired interval size
breaks_number <- (max_value - min_value) / desired_interval_size

hist(eyeradiidf$maxEyeRadii, prob = TRUE, breaks = breaks_number)

lines(density(eyeradiidf$maxEyeRadii), col = "red")


# MAXDISTANCE
distdf <- hurricanedf |>
  filter(POINT_TYPE == "L") |>
  mutate(Max_34KT = pmax(NE_34KT, SE_34KT, NW_34_KT, SW_34_KT)) |>
  summarise(maxwind = max(WINDSPEED_KT), maxEyeRadii = max(EyeRadii), 
            maxdist = max(Max_34KT), .by = c(HID)) |>
  summarise(maxwind = max(maxwind), maxEyeRadii = max(maxEyeRadii), maxdist = max(maxdist), .by = c(HID)) |>
  filter(maxdist != -999 ) |>
  filter(maxdist != 0 ) 


  # ggplot(aes(x = maxdist)) + 
  # geom_histogram(binwidth = 20, color = "white") 


min_value <- min(distdf$maxdist)
max_value <- max(distdf$maxdist)
desired_interval_size <- 20 # Change this to your desired interval size
breaks_number <- (max_value - min_value) / desired_interval_size

hist(distdf$maxdist, prob = TRUE, breaks = breaks_number)

# Overlay a density plot
lines(density(distdf$maxdist), col = "red")


```

```{r}
recenthurdf
```

```{r}
library(ggplot2)
library(maps)
library(dplyr)
library(stringr)

# Load Texas map data
regions <- c("texas", "louisiana")
texas_map <- map_data("state", region = regions)



# Assuming hurricanedf is your hurricane DataFrame
hurricane_plot_data <- hurricanedf %>%
  filter(HNAME == "NICHOLAS") %>%
  filter(year == 2021) %>%
  mutate(lat = as.double(str_sub(LATITUDE, 1, 4))) %>% # Corrected to start at 1 instead of 0
  mutate(long = -as.double(str_sub(LONGITUDE, 1, 4))) |> # Corrected to start at 1 instead of 0
  filter(lat > 25) |>
  mutate(maxkt34 = pmax(NE_34KT, SE_34KT, NW_34_KT, SW_34_KT)) |>
  mutate(maxkt50 = pmax(NE_50KT, SE_50KT, NW_50_KT, SW_50_KT)) |>
  mutate(maxkt64 = pmax(NE_64KT, SE_64KT, NW_64_KT, SW_64_KT)) 

# Plot Texas map and overlay hurricane Nicholas data
ggplot() +
  geom_polygon(data = texas_map, aes(x = long, y = lat, group = group), fill = "lightblue", color = "white") +
  geom_point(data = hurricane_plot_data, aes(x = long, y = lat, size = ifelse(EyeRadii == 0, NA, EyeRadii)), 
             color = "red", alpha = 0.3) +
  geom_point(data = hurricane_plot_data, aes(x = long, y = lat, size = ifelse(maxkt34 == 0, NA, maxkt34)), 
             color = "yellow", alpha = 0.3) +
  geom_point(data = hurricane_plot_data, aes(x = long, y = lat, size = ifelse(maxkt50 == 0, NA, maxkt50)), 
             color = "green", alpha = 0.3) +
  geom_point(data = hurricane_plot_data, aes(x = long, y = lat, size = ifelse(maxkt64 == 0, NA, maxkt64)), 
             color = "blue", alpha = 0.3) +
  scale_size_continuous(name = "Wind Radii (nmi)", range = c(3, 19), limits = c(1, NA)) +  # Ensure 0 values are not plotted
  coord_fixed(1.3) +  # Maintain aspect ratio
  ggtitle("Hurricane Nicholas (2021) Path Over Texas") +
  theme_minimal() +
  theme(legend.position = "none")  # Hide the legend
```

```{r}
library(ggplot2)
library(dplyr)

library(USAboundaries)
library(ggplot2)

# Assuming hurricane_plot_data is your prepared dataset and texas_map is already loaded

# Modified ggplot code
ggplot() +
  geom_polygon(data = texas_map, aes(x = long, y = lat, group = group), fill = "lightblue", color = "white") +
  geom_point(data = hurricane_plot_data, aes(x = long, y = lat, size = ifelse(EyeRadii == 0, NA, EyeRadii)), 
             color = "red", alpha = 0.3) +
  geom_point(data = hurricane_plot_data, aes(x = long, y = lat, size = ifelse(maxkt34 == 0, NA, maxkt34)), 
             color = "yellow", alpha = 0.3) +
  geom_point(data = hurricane_plot_data, aes(x = long, y = lat, size = ifelse(maxkt50 == 0, NA, maxkt50)), 
             color = "green", alpha = 0.3) +
  geom_point(data = hurricane_plot_data, aes(x = long, y = lat, size = ifelse(maxkt64 == 0, NA, maxkt64)), 
             color = "blue", alpha = 0.3) +
  scale_size_continuous(name = "Wind Radii (nmi)", range = c(3, 19), limits = c(1, NA)) +  # Ensure 0 values are not plotted
  coord_fixed(1.3) +  # Maintain aspect ratio
  ggtitle("Hurricane Nicholas (2021) Path Over Texas") +
  theme_minimal() +
  theme(legend.position = "none")  # Hide the legend

```

```{r warning=FALSE}
library(ggplot2)
library(sf)


counties_sf <- us_counties()

states_of_interest <- c("TX")
names(counties_sf) <- make.unique(names(counties_sf), sep = "_")

# Proceed with filtering if duplicates are resolved
counties_filtered <- counties_sf %>%
  filter(state_abbr %in% states_of_interest)
# Assuming counties_filtered is an 'sf' object with appropriate CRS
# and other data preparations are already done

ggplot() +
  geom_sf(data = counties_filtered, fill = 'lightblue', color = "black", size = 0.25, alpha = 0.5) +  # County boundaries
  geom_point(data = hurricane_plot_data, aes(x = long, y = lat, size = ifelse(EyeRadii == 0, NA, EyeRadii)), color = "red", alpha = 0.3) +
  geom_point(data = hurricane_plot_data, aes(x = long, y = lat, size = ifelse(maxkt34 == 0, NA, maxkt34)), color = "yellow", alpha = 0.3) +
  geom_point(data = hurricane_plot_data, aes(x = long, y = lat, size = ifelse(maxkt50 == 0, NA, maxkt50)), color = "green", alpha = 0.3) +
  geom_point(data = hurricane_plot_data, aes(x = long, y = lat, size = ifelse(maxkt64 == 0, NA, maxkt64)), color = "blue", alpha = 0.3) +
  scale_size_continuous(name = "Wind Radii (nmi)", range = c(3, 19), limits = c(1, NA)) +
  coord_sf() +  # Use coord_sf() to integrate sf spatial data
  ggtitle("Hurricane Nicholas (2021) Windfield") +
  theme_minimal() +
  theme(legend.position = "none")


```

```{r}
library(ggplot2)
library(sf)


counties_sf <- us_counties()

states_of_interest <- c("TX")
names(counties_sf) <- make.unique(names(counties_sf), sep = "_")

# Proceed with filtering if duplicates are resolved
counties_filtered <- counties_sf %>%
  filter(state_abbr %in% states_of_interest)
# Assuming counties_filtered is an 'sf' object with appropriate CRS
# and other data preparations are already done

ggplot() +
  geom_sf(data = counties_filtered, fill = 'lightblue', color = "black", size = 0.25, alpha = 0.5) +  # County boundaries
  geom_point(data = hurricane_plot_data, aes(x = long, y = lat, size = ifelse(EyeRadii == 0, NA, EyeRadii)), color = "red", alpha = 0.3) +
  geom_point(data = hurricane_plot_data, aes(x = long, y = lat, size = ifelse(maxkt34 == 0, NA, maxkt34)), color = "yellow", alpha = 0.3) +
  geom_point(data = hurricane_plot_data, aes(x = long, y = lat, size = ifelse(maxkt50 == 0, NA, maxkt50)), color = "green", alpha = 0.3) +
  geom_point(data = hurricane_plot_data, aes(x = long, y = lat, size = ifelse(maxkt64 == 0, NA, maxkt64)), color = "blue", alpha = 0.3) +
  scale_size_continuous(name = "Wind Radii (nmi)", range = c(3, 19), limits = c(1, NA)) +
  coord_sf() +  # Use coord_sf() to integrate sf spatial data
  ggtitle("Hurricane Nicholas (2021) Windfield") +
  theme_minimal() +
  theme(legend.position = "none")
```

```{r}
counties_sf <- us_counties()

states_of_interest <- c("TX")
names(counties_sf) <- make.unique(names(counties_sf), sep = "_")

# Proceed with filtering if duplicates are resolved
counties_filtered <- counties_sf %>%
  filter(state_abbr %in% states_of_interest)
# Assuming counties_filtered is an 'sf' object with appropriate CRS
# and other data preparations are already done

nichodf <- read_csv("NICHOLAS.csv")


hurricane_plot_nicho <- nichodf %>%
  mutate(lat = as.double(str_sub(lat, 1, 4))) %>% # Corrected to start at 1 instead of 0
  mutate(long = -as.double(str_sub(long, 1, 4))) |> # Corrected to start at 1 instead of 0
  filter(lat > 25) |>
  mutate(maxkt34 = pmax(`34kt_ne`, `34kt_se`, `34kt_sw`, `34kt_nw`)) |>
  mutate(maxkt50 = pmax(`50kt_ne`, `50kt_sw`, `50kt_nw`, `50kt_se`)) |>
  mutate(maxkt64 = pmax(`64kt_ne`, `64kt_se`, `64kt_sw`, `64kt_nw`)) 
```

```{r}

  
  
  ggplot() +
  geom_sf(data = counties_filtered, fill = 'lightblue', color = "black", size = 0.25, alpha = 0.5) +  # County boundaries
  geom_point(data = hurricane_plot_nicho, aes(x = long, y = lat, size = ifelse(maxkt34 == 0, NA, maxkt34)), color = "yellow", alpha = 0.3) +
  geom_point(data = hurricane_plot_nicho, aes(x = long, y = lat, size = ifelse(maxkt50 == 0, NA, maxkt50)), color = "green", alpha = 0.3) +
  geom_point(data = hurricane_plot_nicho, aes(x = long, y = lat, size = ifelse(maxkt64 == 0, NA, maxkt64)), color = "blue", alpha = 0.3) +
  scale_size_continuous(name = "Wind Radii (nmi)", range = c(3, 19), limits = c(1, NA)) +
  coord_sf() +  # Use coord_sf() to integrate sf spatial data
  ggtitle("Hurricane Nicholas (2021) Windfield") +
  theme_minimal() +
  theme(legend.position = "none")
```

```{r}
write_csv(panel, "paneldf.csv")
```

```{r}

paneldf <- read_csv("paneldf.csv")

paneldf |>
  filter(STATUS == "HU") |>
  mutate(year = as.double(str_sub(DATE, 0, 4)),
         month = as.double(str_sub(DATE, 5, 6)),
         day = as.double(str_sub(DATE, 7, 8))) |>
  select(year, month) |>
  unique() |>
  summarise(count = n(), .by = year) |>
  ggplot(aes(x = year, y = count)) + 
  geom_line()



paneldf |>
  filter(STATUS == "HU") |>
  mutate(year = as.double(str_sub(DATE, 0, 4)),
         month = as.double(str_sub(DATE, 5, 6)),
         day = as.double(str_sub(DATE, 7, 8))) |>
  select(year, month) |>
  unique() |>
  summarise(count = n(), .by = month) |>
  ggplot(aes(x = month, y = count)) + 
  geom_col() +
  scale_x_discrete(labels = c("Jan", "Feb", "Mar", 
  ))
```

```{r}
library(tidyverse)
library(stringr)

# Assuming paneldf is your dataframe and it has DATE and STATUS columns
# Filter for "HU" status and extract year, month, and day from DATE
monthly_counts <- paneldf |>
  filter(STATUS == "HU") |>
  mutate(year = as.double(str_sub(DATE, 0, 4)),
         month = as.double(str_sub(DATE, 5, 6)),
         day = as.double(str_sub(DATE, 7, 8))) |>
  filter(year >= 1900) |>
  summarise(countprep = n(), .by = c(month, HID)) |>
  summarise(count = n(), .by = month)# Simplified summarise step

# Create a data frame with all months (1 through 12)
all_months <- tibble(month = 1:12)

# Ensure all months are represented by left joining with the all_months dataframe
complete_data <- all_months |>
  left_join(monthly_counts, by = "month") |>
  replace_na(list(count = 0)) # Replace NA counts with 0

# Plot
complete_data |> 
  ggplot(aes(x = factor(month), y = count)) + 
  geom_col(fill = "orange", color = "black") +
  scale_x_discrete(labels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"),
                   breaks = as.character(1:12)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  labs(x = "Months", y = "Hurricane Frequencies") + 
  theme_bw()

```

```{r}
summarydf <- hurricanedf |>
  filter(PRESURE_MB!= -999 & SE_34KT != -999) |>
  select(!c(EyeRadii, year)) |>
  summary()


cols <- colnames(summarydf)
t(sapply(summarydf[cols], summary))
```

```{r}
library(dplyr)
library(tibble)

# Assuming hurricanedf is your dataset and it has been pre-processed to remove -999 values
processed_df <- hurricanedf |>
  filter(PRESURE_MB != -999 & SE_34KT != -999) %>%
  select(-POINT_TYPE, -EyeRadii)

# Function to summarize numeric columns
summarize_numeric <- function(column) {
  c(mean = mean(column, na.rm = TRUE), 
    median = median(column, na.rm = TRUE), 
    non_missing = sum(!is.na(column)))
}

# Function to summarize factor columns
summarize_factor <- function(column) {
  c(unique_values = length(unique(column)),
    most_frequent = names(sort(table(column), decreasing = TRUE)[1]))
}

# Apply the appropriate summarization function to each column and create a summary table
summary_table <- processed_df %>%
  summarise_all(~if(is.numeric(.)) summarize_numeric(.) else summarize_factor(.)) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Summary") %>%
  mutate(Summary = as.character(Summary)) # Convert summaries to character for uniformity

# View the summary table
print(summary_table)

```

```{r}
paneldf |>
  filter(STATUS == "HU") |>
  mutate(year = as.double(str_sub(DATE, 0, 4)),
         month = as.double(str_sub(DATE, 5, 6)),
         day = as.double(str_sub(DATE, 7, 8))) |>
  summarise(countprep = n(), .by = c(year, HID)) |>
  ggplot(aes(x = year)) + 
  geom_histogram(binwidth = 5, fill = "orange",color = "black") + 
  scale_x_continuous(breaks=seq(1850, 2020, by=10)) +
  labs(x = "Year", y = "# of Hurricanes")




paneldf |>
  filter(STATUS == "HU") |>
  mutate(year = as.double(str_sub(DATE, 0, 4)),
         month = as.double(str_sub(DATE, 5, 6)),
         day = as.double(str_sub(DATE, 7, 8))) |>
  filter(year >= 1900) |>
  summarise(countprep = n(), .by = c(year, HID)) |>
  summarise(countfinal = n(), .by = year) |>
  ggplot(aes(x = year, y = countfinal)) +
  geom_point(color = "black") + 
  geom_line(color = "black") + 
  geom_smooth(method="lm", formula= y~x, se=FALSE, color = "red") + 
  labs(x = "year", y = "Number of Hurricanes")

lmyr <- paneldf |>
  filter(STATUS == "HU") |>
  mutate(year = as.double(str_sub(DATE, 0, 4)),
         month = as.double(str_sub(DATE, 5, 6)),
         day = as.double(str_sub(DATE, 7, 8))) |>
  filter(year >= 1900) |>
  summarise(countprep = n(), .by = c(year, HID)) |>
  summarise(countfinal = n(), .by = year) 



model <- lm(formula = countfinal~year, data = lmyr)
   
   summary(model)
   
   
   
```
```{r}
paneldf |>
  filter(STATUS == "HU") |>
  mutate(year = as.double(str_sub(DATE, 0, 4)),
         month = as.double(str_sub(DATE, 5, 6)),
         day = as.double(str_sub(DATE, 7, 8))) |>
  filter(year >= 1900) |>
  summarise(countprep = n(), .by = c(year, HID)) |>
  summarise(countfinal = n(), .by = year)
```
```{r}

# "Harris", "Fort Bend", "Galveston"

TXhurprocss %>%
  filter(state == "Texas") %>%
  filter(county %in% c("Harris")) %>%
  merge(TXpop) %>%
  reframe(outagedaily = max(sum) / population, .by = c(date, county)) %>%
  unique() %>%
  mutate(interval = cut(outagedaily, breaks = c(0, seq(0.01, 0.10, by = 0.001)), 
                        labels = paste0(seq(0, 0.09, by = 0.001), " - ", seq(0.01, 0.10, by = 0.001)))) %>%
  group_by(interval) %>%
  summarise(probability = n() / nrow(.)) %>%
  ggplot(aes(x = interval, y = probability)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(x = "Outage Daily Interval", y = "Probability") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

TXhurprocss %>%
  filter(state == "Texas") %>%
  filter(county %in% c("Harris")) %>%
  merge(TXpop) %>%
  reframe(outagedaily = max(sum) / population, .by = c(date, county)) %>%
  unique() %>%
  mutate(interval = cut(outagedaily, breaks = c(0, seq(0.01, 0.10, by = 0.01)), 
                        labels = paste0(seq(0, 0.09, by = 0.01), " - ", seq(0.01, 0.10, by = 0.01)))) %>%
  group_by(interval) %>%
  summarise(probability = n() / nrow(.)) %>%
  mutate(probability_percentage = scales::percent(probability)) %>%
  ggplot(aes(x = interval, y = 1)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  geom_text(aes(label = probability_percentage), vjust = -0.5, size = 3.5) +
  labs(x = "Outage Daily Interval", y = "Probability") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



library(dplyr)
library(ggplot2)




TXhurprocss %>%
  filter(sum > 0) |>
  filter(state == "Texas") %>%
  filter(county %in% c("Harris")) %>%
  merge(TXpop) %>%
  reframe(outagedaily = max(sum) / population, .by = c(date, county)) %>%
  unique() %>%
  mutate(interval = cut(outagedaily, breaks = 35)) %>%  # Adjust the number of breaks as needed
  reframe(probability = n() / nrow(.), .by = interval) %>%
  mutate(probability_percentage = scales::percent(probability))

```

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)

TXhurprocss %>%
  filter(sum > 1000) %>%
  filter(state == "Texas") %>%
  filter(county %in% c("Brazoria")) %>%
  merge(TXpop) %>%
  reframe(outagedaily = max(sum), .by = c(date, county)) %>%
  unique() %>%
  mutate(interval = cut(outagedaily, breaks = quantile(outagedaily, probs = seq(0, 1, by = 0.1), na.rm = TRUE))) %>%
  reframe(probability = n() / nrow(.), .by = interval) %>%
  mutate(percentage_Brazoria = scales::percent(probability)) |>
  select(!probability)

TXhurprocss %>%
  filter(sum > 1000) %>%
  filter(state == "Texas") %>%
  filter(county %in% c("Harris")) %>%
  merge(TXpop) %>%
  reframe(outagedaily = max(sum), .by = c(date, county)) %>%
  unique() %>%
  mutate(interval = cut(outagedaily, breaks = quantile(outagedaily, probs = seq(0, 1, by = 0.1), na.rm = TRUE))) %>%
  reframe(probability = n() / nrow(.), .by = interval) %>%
  mutate(percentage_Harris = scales::percent(probability)) |>
  select(!probability)

TXhurprocss %>%
  filter(sum > 1000) %>%
  filter(state == "Texas") %>%
  filter(county %in% c("Fort Bend")) %>%
  merge(TXpop) %>%
  reframe(outagedaily = max(sum), .by = c(date, county)) %>%
  unique() %>%
  mutate(interval = cut(outagedaily, breaks = quantile(outagedaily, probs = seq(0, 1, by = 0.1), na.rm = TRUE))) %>%
  reframe(probability = n() / nrow(.), .by = interval) %>%
  mutate(percentage_FortBend = scales::percent(probability)) |>
  select(!probability)

TXhurprocss %>%
  filter(sum > 0) %>%
  filter(state == "Texas") %>%
  filter(county %in% c("Galveston")) %>%
  merge(TXpop) %>%
  reframe(outagedaily = max(sum), .by = c(date, county)) %>%
  unique() %>%
  mutate(interval = cut(outagedaily, breaks = quantile(outagedaily, probs = seq(0, 1, by = 0.1), na.rm = TRUE))) %>%
  reframe(probability = n() / nrow(.), .by = interval) %>%
  mutate(percentage_Galveston = scales::percent(probability)) |>
  select(!probability)
















```
```{r}
library(dplyr)

# Load your dataset (TXhurprocss and TXpop assumed to be loaded)

# Filter, merge, and compute intervals
data <- TXhurprocss %>%
  filter(sum > 0) %>%
  filter(state == "Texas") %>%
  filter(county %in% c("Harris")) %>%
  merge(TXpop) %>%
  reframe(outagedaily = max(sum), .by = c(date, county)) %>%
  unique() %>%
  mutate(interval = cut(outagedaily, breaks = quantile(outagedaily, probs = seq(0, 1, by = 0.1), na.rm = TRUE)))

# Compute probability
data <- data %>%
  group_by(interval) %>%
  summarise(probability = n() / nrow(.)) %>%
  mutate(midpoint = as.numeric(gsub("\\((.*),.*", "\\1", as.character(interval))))

# Calculate the expected value
expected_value <- sum(data$midpoint * data$probability, na.rm = TRUE)

# Print the expected value
print(expected_value)


```










```{r}
library(ggplot2)
library(dplyr)
library(stringr)



paneldf |>
  filter(STATUS == "HU") |>
  mutate(year = as.double(str_sub(DATE, 1, 4)),  # Adjusted indexing to start from 1
         month = as.double(str_sub(DATE, 5, 6)),
         day = as.double(str_sub(DATE, 7, 8))) |>
  filter(year >= 1900) |>
  group_by(year, HID) |>
  summarise(countprep = n(), .groups = "drop") |>
  group_by(year) |>
  summarise(countfinal = n(), .groups = "drop") |>
  ggplot(aes(x = year, y = countfinal)) +
  geom_point(color = "black") + 
  geom_line(color = "black") + 
  geom_smooth(method="lm", formula= y~x, se=FALSE, color = "red") + 
  labs(x = "Year", y = "Number of Hurricanes") +
  scale_x_continuous(breaks = seq(min(1900), max(2020), by = 10),  # Adjust breaks
                     labels = seq(min(1900), max(2020), by = 10))+ # Adjust labels
theme_bw()
```

```{r}
filter(date > '2019-09-15' & date < '2019-09-25' |
         date > '2020-07-20' & date < '2020-08-01' |
         date > '2020-08-20' & date < '2020-09-05' |
         date > '2020-09-10' & date < '2020-09-20' |
         date > '2020-10-05' & date < '2020-10-20' |
         date > '2021-09-10' & date < '2021-09-20' ) 


TXhurprocss |>
  summarise(maxsumdaily = max(sum), .by = c(fips_code, county, date))

windspeedHUR <- hurricanedf |>
  filter(year >= 2019) |>
  arrange(DATE) |>
mutate(
    lat = as.double(str_sub(LATITUDE, 1, -2)), # Extracts all but the last character
    long = as.double(str_sub(LONGITUDE, 1, -2)) # Extracts all but the last character
  ) %>%
  mutate(
    long = if_else(str_ends(LONGITUDE, "W"), -long, long) # Assumes W is negative, E is positive
  ) |>
  filter(lat > 25) |>
  mutate(maxkt34 = pmax(NE_34KT, SE_34KT, NW_34_KT, SW_34_KT)) |>
  mutate(maxkt50 = pmax(NE_50KT, SE_50KT, NW_50_KT, SW_50_KT)) |>
  mutate(maxkt64 = pmax(NE_64KT, SE_64KT, NW_64_KT, SW_64_KT)) |>
  select(HID, year, lat, long, HNAME, DATE, WINDSPEED_KT, maxkt34, maxkt50, maxkt64) |>
  group_by(HID, year, HNAME, DATE) |>
  filter(WINDSPEED_KT == max(WINDSPEED_KT)) |>
  filter(maxkt34 == max(maxkt34)) |>
  ungroup()



TXHURinfo <- windspeedHUR |>
  summarise(
    avglat = mean(lat),
    avglong = mean(long),
    .by = c(HID, year, HNAME, DATE, WINDSPEED_KT, maxkt34, maxkt50, maxkt64)
  ) |>
  filter(avglat >= 25.8, avglat <= 36.5, avglong >= -106.6, avglong <= -93.5) |>
  mutate(date = ymd(DATE)) |>
  select(!DATE)



  
  
  
  
```

```{r}

centercounty <- county_centers |>
  filter(state_name == "Texas") |>
  mutate(fips_code = as.double(fips)) |>
  select(county_name, fips_code, population, latitude, longitude) |>
  mutate(distanceCoast = c(218, 503, 145, 2, 472, 681, 112, 57, 647, 172,
                           126, 477, 41, 196, 133, 183, 477, 264, 359, 16,
                           112, 420, 615, 23, 302, 111, 199, 135, 1, 369,
                           14, 320, 706, 321, 656, 1, 214, 585, 476, 603,
                           374, 325, 371, 628, 73, 144, 304, 310, 433, 218,
                           549, 435, 338, 549, 455, 802, 343, 500, 690, 376,
                           380, 55, 522, 145, 653, 49, 346, 465, 242, 300, 
                           387, 318, 188, 404, 95, 438, 585, 527, 28, 340,
                           217, 134, 529, 5, 510, 212, 422, 40, 92, 708,
                           422, 264, 84, 124, 603, 616, 269, 790, 558, 56,
                           15, 263, 794, 453, 154, 733, 265, 47, 266, 580,
                           324, 347, 165, 444, 417, 356, 739, 357, 412, 20,
                           69, 462, 34, 52, 34, 314, 414, 69, 318, 170,
                           14, 488, 202, 260, 513, 220, 8, 479, 402, 616,
                           223, 107, 73, 115, 170, 34, 210, 780, 57, 212, 
                           512, 561, 527, 279, 222, 80, 131, 291, 457, 249,
                           17, 189, 146, 285, 457, 148, 267, 428, 450, 50,
                           774, 326, 566, 171, 259, 60, 413, 4, 793, 734,
                           30, 369, 222, 360, 676, 415, 99, 708, 413, 326,
                           697, 385, 209, 387, 484, 15, 727, 150, 346, 350,
                           232, 119, 142, 79, 9, 253, 324, 455, 393, 181, 
                           816, 256, 303, 80, 386, 396, 465, 299, 640, 345,
                           388, 354, 545, 429, 340, 347, 154, 128, 84, 289,
                           409, 183, 246, 296, 21, 102, 53, 470, 82, 97,
                           55, 678, 494, 532, 20, 166, 99, 497, 395, 305,
                           558, 418, 94, 161)) |>
  merge(plotfrequencyHUR) |>
  select(!c(fips, frequency, hi))


vulnerbaility <- countieslistTX |>
  merge(fipsTXvulnerM, all = T) 

  
powerplantnum <- countieslistTX |>
  mutate(County = county) |>
  merge(dfdeficit, all = T) |>
  select(!c( hi)) |>
  mutate(totalpower = ifelse(is.na(totalpower), 0, totalpower),
         count = ifelse(is.na(count), 0, count)) 

gdpTXcounty <- gdpTXcounty |>
  mutate(fips_code = as.double(fips))

reredf <- countieslistTX |>
  merge(rere, all = T) |>
  select(!c( hi)) |>
  mutate(resilience = as.double(resilience),
         recover = as.double(recover)) |>
  mutate(resilience = ifelse(resilience == 0, NA, resilience),
         recover = ifelse(recover == 0, NA, recover))


combineTXfinal <- TXhurprocss |>
  merge(centercounty) |>
  filter(sum < population) |>
  summarise(maxsumdaily = max(sum), .by = c(fips_code, county, date)) |>
  merge(centercounty) |>
  merge(reredf) |>
  merge(TXHURinfo) |>
  merge(vulnerbaility, all = T) |> 
  merge(powerplantnum) |>
  merge(gdpTXcounty) |>
  merge(nridf) |>
  select(!hi)
```

```{r}
combineTXtest <- combineTXfinal |>
  mutate(maxsumdaily = if_else(is.na(maxsumdaily), 0, maxsumdaily)) |>
  filter(maxsumdaily == max(maxsumdaily), .by = c(HID, fips_code))
  


```

```{r}
vulnerbaility |>
  arrange(desc(impactPOP))
```

```{r}
write_csv(combineTXfinal, "lgprepDFFINAL.csv" )

combineTXfinal <- read_csv("combineTXfinal.csv")

library(dplyr)
library(geosphere)

lgdf <- combineTXtest %>%
  select(!population) |>
  merge(ULTIMATEdata, all = TRUE) |>
  rowwise() %>%
  mutate(duration = resilience + recover) |>
  mutate(distance = distHaversine(c(longitude, latitude), c(avglong, avglat)) / 1852) %>%
  ungroup() |>
  mutate(eyeradii = maxkt64/2) |>
  mutate(slope64 = (WINDSPEED_KT - 64)/(eyeradii - maxkt64), intercept64 = WINDSPEED_KT-slope64*eyeradii) |>
  mutate(slope50 = (64 - 50)/(maxkt64 - maxkt50), intercept50 = 64-slope50*maxkt64) |>
  mutate(slope34 = (50 - 34)/(maxkt50 - maxkt34), intercept34 = 50-slope34*maxkt50) |>
  mutate(windspeedinfluence = case_when(distance < eyeradii ~ WINDSPEED_KT ,
                                        distance > eyeradii & distance < maxkt64 ~ distance*slope64 + intercept64,
                                        distance > maxkt64 & distance < maxkt50 ~ distance*slope50 + intercept50,
                                        distance > maxkt50 & distance < maxkt34 ~ distance*slope34 + intercept34,
                                        distance > maxkt34 ~ 0)) |>
  mutate(maxsumdaily2 = maxsumdaily/population) |>
  select(county, fips_code, maxsumdaily, vulnerability, resilience, recover, population, AREA, windspeedinfluence, max_rateincrease)


lgdfnew <- lgdf

write_csv(lgdfnew, "lgdfnew.csv")

```

```{r}
lgprepDFFINAL.csv


LGDFtest <- lgdfFINAL |>
  mutate(ab = max_rateincrease*windspeedinfluence) |>
  mutate(cd = vulnerability*resilience ) |>
  mutate(efg = vulnerability * population * windspeedinfluence) |>
  mutate(stability = max_rateincrease/population)

lgdfFINAL <- lgdfnew |>
  drop_na()
```

```{r}
model1 <- lm(LGDFtest, formula = maxsumdaily ~  efg  + max_rateincrease + 
               recover )


               


model2 <- gam(lgdf, formula = maxsumdaily2 ~  windspeedinfluence +  powergridstrength  + 
                resilience + recover, family = gaussian )

summary(model1)

AIC(model2)
BIC(model2)

# For logistic regression, assuming binary response variable
anova(model2, test = "Chisq")

lgdfnew <- lgdf


```

```{r}


write_(lgdfFINAL, "lgdfFINAL.csv")

txt



write.table(lgdfFINAL, "data.txt", append = FALSE, sep = " ", dec = ".", row.names = TRUE, col.names = TRUE)

```


```{r}
lgdf <- read_csv("lgdf.csv")
```

```{r}
vulnerbaility

reredf


```

```{r}
library(rstanarm)
model3 <- stan_glm(lgdf, formula = maxsumdaily2 ~  windspeedinfluence +  powergridstrength  + resilience + recover, family = gaussian)

summary(model3)


sensitivity1 <- lgdf |>
  filter(windspeedinfluence != 0) |>
  filter(fips_code == 48201 | fips_code == 48039 | fips_code == 48047) |>
  filter(windspeedinfluence == max(windspeedinfluence), .by = fips_code) 

|>
  mutate(windspeedinfluence10p = windspeedinfluence * 1.1, windspeedinfluence10n = windspeedinfluence * 0.9) |>
  mutate(powergridstrength10p = powergridstrength * 1.1, powergridstrength10n = powergridstrength * 0.9)  |>
  mutate(resilience10p = resilience * 1.1, resilience10n = resilience * 0.9) |>
  mutate(recover10p = recover * 1.1, recover10n = recover * 0.9)  |>
  
  mutate(one_p = -1.734e-03 + 3.096e-03*windspeedinfluence10p + 
           5.781e-02*powergridstrength + -1.760e-08*resilience + 2.496e-08*recover) |>
  mutate(one_n = -1.734e-03 + 3.096e-03*windspeedinfluence10n + 
         5.781e-02*powergridstrength + -1.760e-08*resilience + 2.496e-08*recover) |>
  
  mutate(two_p = -1.734e-03 + 3.096e-03*windspeedinfluence + 
         5.781e-02*powergridstrength10p + -1.760e-08*resilience + 2.496e-08*recover) |>
  mutate(two_n = -1.734e-03 + 3.096e-03*windspeedinfluence + 
         5.781e-02*powergridstrength10n + -1.760e-08*resilience + 2.496e-08*recover) |>
  
  mutate(three_p = -1.734e-03 + 3.096e-03*windspeedinfluence + 
         5.781e-02*powergridstrength + -1.760e-08*resilience10p + 2.496e-08*recover) |>
  mutate(three_n = -1.734e-03 + 3.096e-03*windspeedinfluence + 
         5.781e-02*powergridstrength + -1.760e-08*resilience10n + 2.496e-08*recover) |>
  
  mutate(four_p = -1.734e-03 + 3.096e-03*windspeedinfluence + 
         5.781e-02*powergridstrength + -1.760e-08*resilience + 2.496e-08*recover10p) |>
  mutate(four_n = -1.734e-03 + 3.096e-03*windspeedinfluence + 
         5.781e-02*powergridstrength + -1.760e-08*resilience + 2.496e-08*recover10n) |>
  
  mutate(allp = -1.734e-03 + 3.096e-03*windspeedinfluence10p + 
         5.781e-02*powergridstrength10p + -1.760e-08*resilience10p + 2.496e-08*recover10p) |>
  
  mutate(alln = -1.734e-03 + 3.096e-03*windspeedinfluence10n + 
         5.781e-02*powergridstrength10n + -1.760e-08*resilience10n + 2.496e-08*recover10n) |>
  
  mutate(orginal = -1.734e-03 + 3.096e-03*windspeedinfluence + 
         5.781e-02*powergridstrength + -1.760e-08*resilience + 2.496e-08*recover) |>
  select(fips_code, orginal, one_p, one_n, two_p, two_n, three_p, three_n, four_p, four_n, allp, alln) 


#Dallas - 48113
#Harris - 48201
#Tarrant - 48439

#Harris - 48201
#Brazoria - 48039
#Galveston - 48047


sensitivity1 |>
  mutate(one_p = (one_p-orginal)*100/orginal) |>
  mutate(one_n = (one_n-orginal)*100/orginal) |>
  mutate(two_p = (two_p-orginal)*100/orginal) |>
  mutate(two_n = (two_n-orginal)*100/orginal) |>
  mutate(three_p = (three_p-orginal)*100/orginal) |>
  mutate(three_n = (three_n-orginal)*100/orginal) |>
  mutate(four_p = (four_p-orginal)*100/orginal) |>
  mutate(four_n = (four_n-orginal)*100/orginal) |>
  mutate(allp = (allp - orginal)*100/orginal) |>
  mutate(alln = (alln - orginal)*100/orginal)
  

counties

summary(model1)

-1.734e-03
3.096e-03
5.781e-02
-1.760e-08
-2.496e-08
```
```{r}
alexsensitivity <- lgdf |>
  filter(windspeedinfluence != 0) |>
  filter(fips_code == 48201 | fips_code == 48039 | county == "Galveston" | fips_code == 48157) |>
  filter(windspeedinfluence == max(windspeedinfluence), .by = fips_code) |>
  mutate(recover = as.double(recover)) |>
  select(county, fips_code, maxsumdaily, vulnerability, population, windspeedinfluence, max_rateincrease, recover)


write_csv(alexsensitivity, "alexsensitivity.csv")
```

```{r}

lgdfcover <- lgdf |>
  drop_na()

insurrancecover <- lgdf |>
  mutate(maxVULN = max(lgdfcover$vulnerability)) |>
  filter(county == "Brazoria" | county == "Fort Bend" | county == "Galveston" | county == "Harris") |>
  select(county, fips_code, vulnerability, maxVULN) |>
  unique()

coverageinsurrancedf <- TXhurprocss |>
  filter(county == "Brazoria" | county == "Fort Bend" | county == "Galveston" | county == "Harris") |>
  filter(date > '2021-09-11' & date < '2021-09-20') |>
  arrange(desc(date)) |>
  mutate(datetime = as.POSIXct(paste(date, time), 
                               format = "%Y-%m-%d %H:%M:%S")) |>
  arrange(datetime) |>
  filter(sum >= 1000) |>
  mutate(date_start = min(datetime),
         date_end = max(datetime),
         .by = c(county, fips_code)) |>
  reframe(duration = date_end - date_start, .by = c(county, fips_code)) |>
  unique() |>
  mutate(duration = as.numeric(duration)) |>
  merge(insurrancecover)
  

coverageinsurrancedf |>
  mutate(coverageDay = (vulnerability + duration)/(1.7*(maxVULN + duration))) |>
  mutate(coverageHour = (vulnerability + duration*24)/(1.7*(maxVULN + duration*24))) |>
  mutate(coverageMinute = (vulnerability + duration*24*60)/(1.7*(maxVULN + duration*24*60)))

write_csv(coverageinsurrancedf, "coverageinsurrancedf.csv")

```


```{r}
summary(model1)
```

```{r}
library(dplyr)
library(broom)

# Assuming 'lgdf' is your dataframe and it's already loaded in R environment.

# Perform the operations as per the provided snippet
analysis_df <- lgdf %>%
  select(fips_code, maxsumdaily2, windspeedinfluence, powergridstrength, resilience, recover) %>%
  filter(windspeedinfluence != 0) %>%
  filter(fips_code == 48201 | fips_code == 48039 ) %>%
  group_by(fips_code) %>%
  filter(windspeedinfluence == max(windspeedinfluence)) %>%
  mutate(windspeedinfluence10p = windspeedinfluence * 1.1, 
         windspeedinfluence10n = windspeedinfluence * 0.9,
         powergridstrength10p = powergridstrength * 1.1, 
         powergridstrength10n = powergridstrength * 0.9,
         resilience10p = resilience * 1.1, 
         resilience10n = resilience * 0.9,
         recover10p = recover * 1.1, 
         recover10n = recover * 0.9) %>%
  ungroup() # Ungroup data for further analysis

# Fit the linear model to the original data
original_model <- lm(maxsumdaily2 ~ windspeedinfluence + powergridstrength + resilience + recover, data=lgdf)

# Tidy the original model's output
tidy_original_model <- tidy(original_model)

# Print the tidy model's output
print(tidy_original_model)

# Apply the changes to the dataframe for sensitivity analysis
# For example, increasing the 'windspeedinfluence' by 10%
lgdf$windspeedinfluence <- lgdf$windspeedinfluence * 1.1

# Fit the model to the new data
adjusted_model <- lm(maxsumdaily2 ~ windspeedinfluence + powergridstrength + resilience + recover, data=lgdf)

# Tidy the adjusted model's output
tidy_adjusted_model <- tidy(adjusted_model)

# Print the tidy model's output after adjustment
print(tidy_adjusted_model)

# Compare the coefficients from the original and adjusted models
# to assess the impact of the 10% increase in 'windspeedinfluence'

write_csv(analysis_df, "analysis_df.csv")

```
